Находится в del6 virtualenv

Эта версия запускается так:

sudo -s   #requirement of "import keyboard"
. /home/ai/del6/bin/activate
cd ~/del6/ray/rllib/examples
python3.5 custom_env_3.py #first experiment
python3.5 custom_env_4.py #second experiment
python3.5 popugaj.py #third experiment

=== Первый эксперимент.
В первом эксперименте оператор успешно обучает DQN подавать правильные команды, а именно:
-- из "нижнего" (закрытого) положения камеры подавать команду "up"
-- из "верхнего" (открытого) положения камеры подавать команду бездействия "---"
-- не подавать команду "down" (закрывающую камеру).
В пределах одной минуты всё тренируется. Проверено многократно.
По умолчанию стоит на автоматическом режиме обучения. Переключение в ручной доступно в файле custom_env_3.py.

На вход подаётся изображение с веб-камеры.
Реворд DQN = средняя освещенность камеры.
На команду "up" оператор открывает камеру, на команду "down" закрывает (опускает её так, что она смотрит в стол параллельно полу).

Кроме того, агент может сохранять experience memory  в бинарном файле  del6/experience/experience.experience (но это выключено),
и может сохранять часть experience memory в текстовом файле  del6/experience/experience.txt (но это выключено),

Агент предсказывает совершенно правильные q_values для gamma=0 и для gamma=0.5 (нетривиальный случай, но всё равно правильно).

В случае gamma=0.5 правильные q_values [0.2 1 0.8] из открытого состояния и [0.2 0.4 0.8] из закрытого, поскольку:
(1) открытое состояние:
(1а) down => up => --- => --- => ...  даёт -0.2+0.15+0.125+... = 0.2
и так далее.

=== Второй эксперимент.
Второй эксперимент отличается от первого только файлом custom_env_4.py.
Во втором эксперименте оператор обучает DQN чередовать гласную букву "а" с согласными "п" и "м".
Возможных действий агента -- три: сказать "м", "а", "п".

Изначально мы его награждаем клавишей 1-9  (1 это максимально отрицательный, а 9 это максимально положительный реворд).
После промпта "your reward?" засчитывается только первая нажатая клавиша, которая и означает реворд.
Если этой клавишей является "s", то агент идет "спать", т.е. обучается в автоматическом режиме на набранном experience.
Чтобы "разбудить" его (вернуть его в ручной режим обучения), нажать клавишу "w".
Также, во время сна доступна пауза - клавишей "c", а выйти из паузы - клавишей "d".

За несколько минут, и несколько переключений в сон, он обучается чередовать гласные с согласными.

=== Третий эксперимент.
Мы что-то говорим в микрофон, а он распознаёт это и говорит обратно своим голосом.
Используется библиотека speech_recognition и gtts,
Рекомендуется после того, как фраза произнесена, уменьшать на время чувствительность микрофона (Capture Devices => Built-in Audio Analog Stereo), чтобы агент понял, что фраза закончилась.