Находится в del6 virtualenv

Эта версия запускается так:

. /home/ai/del6/bin/activate
cd ~/del6/ray/rllib/examples
python3.5 custom_env_3.py #first experiment
python3.5 custom_env_4.py #second experiment

=== Первый эксперимент.
В первом эксперименте оператор успешно обучает DQN подавать правильные команды, а именно:
-- из "нижнего" (закрытого) положения камеры подавать команду "up"
-- из "верхнего" (открытого) положения камеры подавать команду бездействия "---"
-- не подавать команду "down" (закрывающую камеру).
В пределах одной минуты всё тренируется. Проверено многократно.
По умолчанию стоит на автоматическом режиме обучения. Переключение в ручной доступно в файле custom_env_3.py.

На вход подаётся изображение с веб-камеры.
Реворд DQN = средняя освещенность камеры.
На команду "up" оператор открывает камеру, на команду "down" закрывает (опускает её так, что она смотрит в стол параллельно полу).

Кроме того, агент может сохранять experience memory  в бинарном файле  del6/experience/experience.experience (но это выключено),
и может сохранять часть experience memory в текстовом файле  del6/experience/experience.txt (но это выключено),

Агент предсказывает совершенно правильные q_values для gamma=0 и для gamma=0.5 (нетривиальный случай, но всё равно правильно).

В случае gamma=0.5 правильные q_values [0.2 1 0.8] из открытого состояния и [0.2 0.4 0.8] из закрытого, поскольку:
(1) открытое состояние:
(1а) down => up => --- => --- => ...  даёт -0.2+0.15+0.125+... = 0.2
и так далее.

=== Второй эксперимент.
Второй эксперимент отличается от первого только файлом custom_env_4.py.
Во втором эксперименте оператор обучает DQN чередовать гласную букву "а" с согласными "п" и "м".
Возможных действий агента -- три: сказать "м", "а", "п".
В автоматическом режиме всё хорошо получается. За несколько сотен обучающих примеров агент всегда (кроме exploration) чередует "а" с согласными.

TODO: Достаточно набрать несколько десятков обучающих примеров, чтобы агент увидел реворды в ответ все возможные 3*3=9 вариантов сочетания двух символов. При начальном exploration ~100% это легко и быстро наберется.
Потом он уже в автономном режиме сможет натренировать нейросеть.
В автономный режим он перейдёт по нашей команде.
То есть, всего минуту мы его обучаем... потом переводим в автономный режим, 1-2 минуты ждём, и он уже всё умеет.